AWSTemplateFormatVersion: '2010-09-09'
Description: Scheduled EC2 instance scaling based on time using Lambda and tags.

Parameters:
  LambdaFunctionName:
    Type: String
    Default: DynamicEC2Scheduler
  LambdaScheduleUpTime:
    Type: String
    Default: "cron(0 12 ? * MON-FRI *)"
  LambdaScheduleDownTime:
    Type: String
    Default: "cron(0 3 ? * TUE-SAT *)"
  BusinessHoursScheduleUpTime:
    Type: String
    Default: "cron(0 17 ? * MON-FRI *)"
    Description: Cron expression that triggers the business-hours scale up (default 9 AM Pacific).
  BusinessHoursScheduleDownTime:
    Type: String
    Default: "cron(0 2 ? * TUE-SAT *)"
    Description: Cron expression that triggers the business-hours scale down (default 6 PM Pacific).
  ScheduleTagKey:
    Type: String
    Default: DynamicScalingSchedule
    Description: Tag key used to assign instances to alternative scaling schedules.
  OffHoursInstanceType:
    Type: String
    Default: t3.medium
    Description: Instance type to apply during scale-down operations when downsizing instances.
  ConcurrentInstanceOperations:
    Type: Number
    Default: 4
    MinValue: 1
    MaxValue: 20
    Description: Maximum number of EC2 instances to process concurrently.
  SavingsMetricNamespace:
    Type: String
    Default: "DynamicEC2Scaler/Savings"
    Description: CloudWatch metric namespace to use when publishing savings data.
  SavingsPlanDiscountPercent:
    Type: Number
    Default: 0
    MinValue: 0
    MaxValue: 100
    Description: Percentage discount applied to On-Demand rates by an account-wide Compute Savings Plan (0-100).
  SavingsPlanDiscountMode:
    Type: String
    Default: Manual
    AllowedValues:
      - Manual
      - Coverage
    Description: Controls how Savings Plan discounts are applied (Manual uses the provided percentage, Coverage derives an effective rate from Cost Explorer).
  SavingsPlanCoverageLookbackDays:
    Type: Number
    Default: 30
    MinValue: 1
    MaxValue: 90
    Description: Lookback window (in days) to use when deriving Savings Plan coverage data from Cost Explorer.
  DefaultPricingOperatingSystem:
    Type: String
    Default: Linux
    AllowedValues:
      - Linux
      - Windows
      - RHEL
      - SUSE
    Description: Default operating system filter to use for pricing lookups when an instance platform cannot be detected.
  DefaultPricingLicenseModel:
    Type: String
    Default: "No License required"
    Description: Default license model filter to use for pricing lookups when an instance platform cannot be detected.
  DefaultPricingPreInstalledSoftware:
    Type: String
    Default: NA
    Description: Default pre-installed software filter to use for pricing lookups when an instance platform cannot be detected.

Resources:

  EC2ScalerLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunctionName}"
      RetentionInDays: 14

  EC2ScalerRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: EC2ScalerLambdaRole
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: EC2ScalerPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                  - ec2:DescribeTags
                  - pricing:GetProducts
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - kms:CreateGrant
                Resource: "*"
              - Effect: Allow
                Action:
                  - ec2:StopInstances
                  - ec2:StartInstances
                  - ec2:ModifyInstanceAttribute
                  - ec2:CreateTags
                Resource: "*"
                Condition:
                  StringEquals:
                    ec2:ResourceTag/DynamicInstanceScaling: "true"
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub "arn:${AWS::Partition}:s3:::${SavingsLogBucket}/*"
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: "*"
              - Effect: Allow
                Action:
                  - ce:GetSavingsPlansCoverage
                Resource: "*"

  EC2ScalerFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Ref LambdaFunctionName
      Description: Scales EC2 instances up or down based on schedule and tags
      Runtime: python3.12
      Role: !GetAtt EC2ScalerRole.Arn
      Handler: index.lambda_handler
      MemorySize: 512
      ReservedConcurrentExecutions: 10
      Timeout: 300
      Environment:
        Variables:
          SAVINGS_BUCKET: !Ref SavingsLogBucket
          SAVINGS_PLAN_DISCOUNT_PERCENT: !Ref SavingsPlanDiscountPercent
          SAVINGS_PLAN_DISCOUNT_MODE: !Ref SavingsPlanDiscountMode
          SAVINGS_PLAN_COVERAGE_LOOKBACK_DAYS: !Ref SavingsPlanCoverageLookbackDays
          SAVINGS_METRIC_NAMESPACE: !Ref SavingsMetricNamespace
          OFF_HOURS_INSTANCE_TYPE: !Ref OffHoursInstanceType
          DEFAULT_PRICING_OPERATING_SYSTEM: !Ref DefaultPricingOperatingSystem
          DEFAULT_PRICING_LICENSE_MODEL: !Ref DefaultPricingLicenseModel
          DEFAULT_PRICING_PREINSTALLED_SOFTWARE: !Ref DefaultPricingPreInstalledSoftware
          SCALE_UP_CRON_EXPRESSION: !Ref LambdaScheduleUpTime
          SCALE_DOWN_CRON_EXPRESSION: !Ref LambdaScheduleDownTime
          SCHEDULE_TAG_KEY: !Ref ScheduleTagKey
          MAX_CONCURRENT_OPERATIONS: !Ref ConcurrentInstanceOperations
      Code:
        ZipFile: |
          import boto3
          import datetime
          import json
          import os
          import threading
          import time
          from concurrent.futures import ThreadPoolExecutor, as_completed

          session = boto3.session.Session()
          s3 = session.client("s3")
          cloudwatch = session.client("cloudwatch")
          cost_explorer = session.client("ce", region_name="us-east-1")
          _thread_local = threading.local()
          PRICE_CACHE = {}
          PRICE_CACHE_LOCK = threading.Lock()
          SAVINGS_PLAN_FACTOR = None
          SAVINGS_PLAN_FACTOR_SOURCE = None
          SAVINGS_PLAN_LOCK = threading.Lock()
          DEFAULT_METRIC_NAMESPACE = "DynamicEC2Scaler/Savings"

          DEFAULT_DOWNSIZE_TYPE = "t3.medium"
          VALID_ACTIONS = {"scaleup", "scaledown"}
          RUN_CONTEXT = {}
          LAST_SCALE_DOWN_TIMESTAMP_TAG = "DynamicScalingLastScaleDownTimestamp"
          LAST_SCALE_DOWN_HOURLY_TAG = "DynamicScalingLastScaleDownHourlySavings"
          LAST_SCALE_UP_TIMESTAMP_TAG = "DynamicScalingLastScaleUpTimestamp"
          SCALE_UP_CRON_ENV = "SCALE_UP_CRON_EXPRESSION"
          DEFAULT_SCHEDULE_NAME = "default"
          SCHEDULE_ALL_TOKEN = "all"

          REGION_NAME_MAP = {
              "af-south-1": "Africa (Cape Town)",
              "ap-east-1": "Asia Pacific (Hong Kong)",
              "ap-northeast-1": "Asia Pacific (Tokyo)",
              "ap-northeast-2": "Asia Pacific (Seoul)",
              "ap-northeast-3": "Asia Pacific (Osaka)",
              "ap-south-1": "Asia Pacific (Mumbai)",
              "ap-south-2": "Asia Pacific (Hyderabad)",
              "ap-southeast-1": "Asia Pacific (Singapore)",
              "ap-southeast-2": "Asia Pacific (Sydney)",
              "ap-southeast-3": "Asia Pacific (Jakarta)",
              "ap-southeast-4": "Asia Pacific (Melbourne)",
              "ap-southeast-5": "Asia Pacific (Malaysia)",
              "ca-central-1": "Canada (Central)",
              "ca-west-1": "Canada West (Calgary)",
              "eu-central-1": "EU (Frankfurt)",
              "eu-central-2": "EU (Zurich)",
              "eu-north-1": "EU (Stockholm)",
              "eu-south-1": "EU (Milan)",
              "eu-south-2": "EU (Spain)",
              "eu-west-1": "EU (Ireland)",
              "eu-west-2": "EU (London)",
              "eu-west-3": "EU (Paris)",
              "il-central-1": "Israel (Tel Aviv)",
              "me-central-1": "Middle East (UAE)",
              "me-south-1": "Middle East (Bahrain)",
              "sa-east-1": "South America (SÃ£o Paulo)",
              "us-east-1": "US East (N. Virginia)",
              "us-east-2": "US East (Ohio)",
              "us-west-1": "US West (N. California)",
              "us-west-2": "US West (Oregon)",
          }

          CRON_WEEKDAY_MAP = {
              "MON": 0,
              "TUE": 1,
              "WED": 2,
              "THU": 3,
              "FRI": 4,
              "SAT": 5,
              "SUN": 6,
          }

          def get_ec2_client():
              client = getattr(_thread_local, "ec2", None)
              if client is None:
                  client = session.client("ec2")
                  _thread_local.ec2 = client
              return client

          def get_pricing_client():
              client = getattr(_thread_local, "pricing", None)
              if client is None:
                  client = session.client("pricing", region_name="us-east-1")
                  _thread_local.pricing = client
              return client

          def get_schedule_tag_key():
              raw_key = os.environ.get("SCHEDULE_TAG_KEY", "DynamicScalingSchedule")
              key = raw_key.strip()
              return key or "DynamicScalingSchedule"

          def normalize_schedule_name(value):
              if value is None:
                  return DEFAULT_SCHEDULE_NAME
              normalized = value.strip().lower()
              if not normalized:
                  return DEFAULT_SCHEDULE_NAME
              return normalized

          def parse_schedule_tag_value(raw_value):
              if not raw_value:
                  return []
              values = []
              for token in raw_value.split(","):
                  cleaned = token.strip().lower()
                  if cleaned:
                      values.append(cleaned)
              return values

          def instance_matches_schedule(tags, schedule_name):
              if schedule_name == SCHEDULE_ALL_TOKEN:
                  return True
              schedule_tag_key = get_schedule_tag_key()
              raw_value = tags.get(schedule_tag_key)
              if not raw_value:
                  return schedule_name == DEFAULT_SCHEDULE_NAME
              values = parse_schedule_tag_value(raw_value)
              if not values:
                  return schedule_name == DEFAULT_SCHEDULE_NAME
              if SCHEDULE_ALL_TOKEN in values:
                  return True
              return schedule_name in values

          def get_concurrency_limit():
              raw_value = os.environ.get("MAX_CONCURRENT_OPERATIONS")
              if raw_value is None:
                  return 4
              try:
                  value = int(str(raw_value).strip())
              except (TypeError, ValueError):
                  return 4
              if value < 1:
                  return 1
              if value > 20:
                  return 20
              return value

          def wait_for_instance_type(client, instance_id, desired_type, timeout_seconds=300, poll_interval=5):
              print(f"Waiting for {instance_id} instance type to update to {desired_type}...")
              deadline = time.time() + timeout_seconds
              while time.time() < deadline:
                  response = client.describe_instances(InstanceIds=[instance_id])
                  reservations = response.get("Reservations", [])
                  for reservation in reservations:
                      for inst in reservation.get("Instances", []):
                          current_type = inst.get("InstanceType")
                          if current_type == desired_type:
                              print(f"{instance_id} instance type confirmed as {desired_type}.")
                              return
                  remaining = deadline - time.time()
                  if remaining <= 0:
                      break
                  time.sleep(min(poll_interval, max(remaining, 1)))
              raise TimeoutError(
                  f"Instance {instance_id} did not report instance type {desired_type} within {timeout_seconds} seconds."
              )

          def format_utc(dt):
              if not isinstance(dt, datetime.datetime):
                  raise TypeError("Expected datetime instance when formatting timestamp")
              if dt.tzinfo is not None:
                  dt = dt.astimezone(datetime.timezone.utc).replace(tzinfo=None)
              return dt.replace(microsecond=0).isoformat() + "Z"

          def set_run_start_time(dt):
              RUN_CONTEXT["start_time"] = dt

          def get_run_start_time():
              return RUN_CONTEXT.get("start_time")

          def parse_timestamp(value):
              if not value:
                  return None
              cleaned = value.strip()
              if not cleaned:
                  return None
              if cleaned.endswith("Z"):
                  cleaned = cleaned[:-1] + "+00:00"
              try:
                  parsed = datetime.datetime.fromisoformat(cleaned)
              except ValueError:
                  return None
              if parsed.tzinfo is not None:
                  parsed = parsed.astimezone(datetime.timezone.utc).replace(tzinfo=None)
              return parsed

          def cron_value_to_weekday(token):
              upper_token = token.strip().upper()
              if not upper_token:
                  raise ValueError("Empty day-of-week token in cron expression")
              if upper_token in CRON_WEEKDAY_MAP:
                  return CRON_WEEKDAY_MAP[upper_token]
              try:
                  numeric = int(token)
              except ValueError as exc:
                  raise ValueError(f"Unsupported day-of-week token: {token}") from exc
              if numeric in (0, 7):
                  return 6
              if 1 <= numeric <= 6:
                  return numeric - 1
              raise ValueError(f"Day-of-week value out of range: {token}")

          def parse_day_of_week_field(field):
              normalized = (field or "*").strip().upper()
              if normalized in {"*", "?"}:
                  return None
              allowed = set()
              for segment in normalized.split(","):
                  segment = segment.strip()
                  if not segment:
                      continue
                  if "-" in segment:
                      start_token, end_token = segment.split("-", 1)
                      start = cron_value_to_weekday(start_token)
                      end = cron_value_to_weekday(end_token)
                      if start <= end:
                          for value in range(start, end + 1):
                              allowed.add(value)
                      else:
                          for value in list(range(start, 7)) + list(range(0, end + 1)):
                              allowed.add(value)
                  else:
                      allowed.add(cron_value_to_weekday(segment))
              return allowed

          def parse_single_int_field(value, field_name, minimum, maximum):
              cleaned = (value or "").strip()
              if cleaned in {"*", "?"}:
                  raise ValueError(
                      f"{field_name} field must be a fixed numeric value in cron expression"
                  )
              try:
                  numeric = int(cleaned)
              except ValueError as exc:
                  raise ValueError(
                      f"{field_name} field must be an integer in cron expression"
                  ) from exc
              if numeric < minimum or numeric > maximum:
                  raise ValueError(
                      f"{field_name} field value {numeric} is out of the allowed range {minimum}-{maximum}"
                  )
              return numeric

          def parse_cron_expression(cron_expression):
              if not cron_expression:
                  raise ValueError("Cron expression is empty")
              expression = cron_expression.strip()
              if expression.lower().startswith("cron(") and expression.endswith(")"):
                  expression = expression[5:-1].strip()
              parts = expression.split()
              if len(parts) != 6:
                  raise ValueError(f"Cron expression must have 6 fields: {cron_expression}")
              (
                  minute_field,
                  hour_field,
                  day_of_month_field,
                  month_field,
                  day_of_week_field,
                  year_field,
              ) = parts
              minute = parse_single_int_field(minute_field, "minute", 0, 59)
              hour = parse_single_int_field(hour_field, "hour", 0, 23)
              if day_of_month_field not in {"*", "?"}:
                  raise ValueError(
                      "Day-of-month field must be '*' or '?' for supported cron expressions"
                  )
              if month_field not in {"*"}:
                  raise ValueError("Month field must be '*' for supported cron expressions")
              if year_field not in {"*"}:
                  raise ValueError("Year field must be '*' for supported cron expressions")
              allowed_days = parse_day_of_week_field(day_of_week_field)
              return minute, hour, allowed_days

          def get_next_scheduled_time(cron_expression, reference_time):
              minute, hour, allowed_days = parse_cron_expression(cron_expression)
              baseline = reference_time.replace(second=0, microsecond=0)
              for day_offset in range(0, 14):
                  candidate_date = baseline.date() + datetime.timedelta(days=day_offset)
                  candidate = datetime.datetime.combine(
                      candidate_date,
                      datetime.time(hour=hour, minute=minute),
                  )
                  if candidate <= reference_time:
                      continue
                  if allowed_days is not None and candidate.weekday() not in allowed_days:
                      continue
                  return candidate
              raise ValueError(
                  "Unable to find the next scheduled time within 14 days for cron expression: "
                  f"{cron_expression}"
              )

          def compute_projected_savings(total_hourly_savings, reference_time):
              schedule_expression = os.environ.get(SCALE_UP_CRON_ENV)
              if not schedule_expression:
                  raise ValueError("Scale up cron expression environment variable is not set")
              next_scale_up = get_next_scheduled_time(schedule_expression, reference_time)
              duration_hours = max((next_scale_up - reference_time).total_seconds() / 3600.0, 0.0)
              projected_total = round(total_hourly_savings * duration_hours, 4)
              return {
                  "projection_source": "scale_up_schedule",
                  "projected_scale_up_time_utc": format_utc(next_scale_up),
                  "projected_off_hours_duration_hours": round(duration_hours, 4),
                  "projected_total_savings": projected_total,
              }

          DEFAULT_PRICING_FILTERS = {
              "operatingSystem": os.environ.get("DEFAULT_PRICING_OPERATING_SYSTEM", "Linux"),
              "preInstalledSw": os.environ.get("DEFAULT_PRICING_PREINSTALLED_SOFTWARE", "NA"),
              "licenseModel": os.environ.get("DEFAULT_PRICING_LICENSE_MODEL", "No License required"),
          }

          PLATFORM_FILTER_RULES = [
              (
                  "windows with sql server enterprise",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "SQL Server Enterprise",
                      "licenseModel": "License Included",
                  },
              ),
              (
                  "windows with sql server standard",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "SQL Server Standard",
                      "licenseModel": "License Included",
                  },
              ),
              (
                  "windows with sql server web",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "SQL Server Web",
                      "licenseModel": "License Included",
                  },
              ),
              (
                  "windows",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "NA",
                      "licenseModel": "License Included",
                  },
              ),
              (
                  "bring your own license",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "NA",
                      "licenseModel": "Bring your own license",
                  },
              ),
              (
                  "byol",
                  {
                      "operatingSystem": "Windows",
                      "preInstalledSw": "NA",
                      "licenseModel": "Bring your own license",
                  },
              ),
              (
                  "red hat enterprise linux",
                  {
                      "operatingSystem": "RHEL",
                      "preInstalledSw": "NA",
                      "licenseModel": "No License required",
                  },
              ),
              (
                  "rhel",
                  {
                      "operatingSystem": "RHEL",
                      "preInstalledSw": "NA",
                      "licenseModel": "No License required",
                  },
              ),
              (
                  "suse",
                  {
                      "operatingSystem": "SUSE",
                      "preInstalledSw": "NA",
                      "licenseModel": "No License required",
                  },
              ),
              (
                  "linux",
                  {
                      "operatingSystem": "Linux",
                      "preInstalledSw": "NA",
                      "licenseModel": "No License required",
                  },
              ),
          ]

          def get_region():
              region_name = session.region_name or os.environ.get("AWS_REGION") or os.environ.get("AWS_DEFAULT_REGION")
              if not region_name:
                  raise ValueError("AWS region could not be determined for pricing lookup")
              return region_name

          def get_location(region_name):
              location = REGION_NAME_MAP.get(region_name)
              if not location:
                  supported_regions = ", ".join(sorted(REGION_NAME_MAP))
                  raise ValueError(
                      "Unsupported region for pricing lookup: "
                      f"{region_name}. Supported regions: {supported_regions}"
                  )
              return location

          def build_pricing_cache_key(instance_type, pricing_filters):
              return (
                  instance_type,
                  pricing_filters.get("operatingSystem"),
                  pricing_filters.get("preInstalledSw"),
                  pricing_filters.get("licenseModel"),
              )

          def get_instance_pricing_profile(instance):
              platform_candidates = [
                  instance.get("PlatformDetails"),
                  instance.get("Platform"),
                  instance.get("UsageOperation"),
              ]

              for platform_text in platform_candidates:
                  normalized = (platform_text or "").strip().lower()
                  if not normalized:
                      continue
                  for match, filters in PLATFORM_FILTER_RULES:
                      if match in normalized:
                          return dict(filters), f"platform:{platform_text}"

              return dict(DEFAULT_PRICING_FILTERS), "default"

          def get_hourly_rate(instance_type, pricing_filters):
              cache_key = build_pricing_cache_key(instance_type, pricing_filters)
              with PRICE_CACHE_LOCK:
                  cached_price = PRICE_CACHE.get(cache_key)
              if cached_price is not None:
                  return cached_price

              region_name = get_region()
              location = get_location(region_name)
              pricing_client = get_pricing_client()

              def build_filters():
                  attribute_filters = []
                  for field in ("operatingSystem", "preInstalledSw", "licenseModel"):
                      value = pricing_filters.get(field)
                      if value:
                          attribute_filters.append(
                              {"Type": "TERM_MATCH", "Field": field, "Value": value}
                          )

                  base_filters = [
                      {"Type": "TERM_MATCH", "Field": "instanceType", "Value": instance_type},
                      {"Type": "TERM_MATCH", "Field": "location", "Value": location},
                  ]
                  base_filters.extend(attribute_filters)
                  base_filters.extend(
                      [
                          {"Type": "TERM_MATCH", "Field": "tenancy", "Value": "Shared"},
                          {"Type": "TERM_MATCH", "Field": "capacitystatus", "Value": "Used"},
                      ]
                  )
                  return base_filters

              def remove_filter(filters, field_name):
                  return [
                      filter_item
                      for filter_item in filters
                      if filter_item.get("Field") != field_name
                  ]

              def update_filter(filters, field_name, value):
                  updated = []
                  for filter_item in filters:
                      if filter_item.get("Field") == field_name:
                          updated.append({**filter_item, "Value": value})
                      else:
                          updated.append(dict(filter_item))
                  return updated

              def serialize_filters(filters):
                  return tuple(
                      sorted(
                          (item.get("Field"), item.get("Value"))
                          for item in filters
                      )
                  )

              def format_filters(filters):
                  parts = []
                  for item in filters:
                      field = item.get("Field")
                      value = item.get("Value")
                      if field and value:
                          parts.append(f"{field}={value}")
                  return ", ".join(parts)

              base_filters = build_filters()
              filter_variants = [base_filters]

              license_value = pricing_filters.get("licenseModel")
              if license_value:
                  if license_value == "License Included":
                      filter_variants.append(
                          update_filter(base_filters, "licenseModel", "No License required")
                      )
                  filter_variants.append(remove_filter(base_filters, "licenseModel"))

              preinstalled_value = pricing_filters.get("preInstalledSw")
              if preinstalled_value:
                  filter_variants.append(remove_filter(base_filters, "preInstalledSw"))

              if license_value and preinstalled_value:
                  filter_variants.append(
                      remove_filter(
                          remove_filter(base_filters, "licenseModel"),
                          "preInstalledSw",
                      )
                  )

              unique_variants = []
              seen_variants = set()
              for candidate in filter_variants:
                  signature = serialize_filters(candidate)
                  if signature not in seen_variants:
                      seen_variants.add(signature)
                      unique_variants.append(candidate)

              attempted_filters = []

              def try_filters(filters_to_use):
                  response = pricing_client.get_products(
                      ServiceCode="AmazonEC2",
                      Filters=filters_to_use,
                      MaxResults=1,
                  )
                  price_list = response.get("PriceList")
                  attempted_filters.append(format_filters(filters_to_use))
                  if not price_list:
                      return None

                  price_item = json.loads(price_list[0])
                  on_demand_terms = price_item["terms"].get("OnDemand", {})
                  for term in on_demand_terms.values():
                      price_dimensions = term.get("priceDimensions", {})
                      for dimension in price_dimensions.values():
                          return float(dimension["pricePerUnit"].get("USD", "0"))
                  return None

              for filters_to_use in unique_variants:
                  price = try_filters(filters_to_use)
                  if price is not None:
                      with PRICE_CACHE_LOCK:
                          PRICE_CACHE[cache_key] = price
                      return price

              attempted_summary = "; ".join(attempted_filters) or "(none)"
              raise ValueError(
                  "No pricing information found for "
                  f"{instance_type} ({pricing_filters}) in {region_name} "
                  f"using filters: {attempted_summary}"
              )

          def parse_float(value, default=0.0):
              try:
                  return float(value)
              except (TypeError, ValueError):
                  return default

          def get_manual_discount_factor():
              raw_value = os.environ.get("SAVINGS_PLAN_DISCOUNT_PERCENT", "0")
              try:
                  discount_percent = float(raw_value)
              except ValueError as exc:
                  raise ValueError(
                      "SAVINGS_PLAN_DISCOUNT_PERCENT must be a number between 0 and 100"
                  ) from exc

              if discount_percent < 0 or discount_percent > 100:
                  raise ValueError(
                      "SAVINGS_PLAN_DISCOUNT_PERCENT must be between 0 and 100"
                  )

              return 1 - (discount_percent / 100.0)

          def get_coverage_discount_factor():
              raw_lookback = os.environ.get("SAVINGS_PLAN_COVERAGE_LOOKBACK_DAYS", "30")
              try:
                  lookback_days = int(raw_lookback)
              except ValueError as exc:
                  raise ValueError(
                      "SAVINGS_PLAN_COVERAGE_LOOKBACK_DAYS must be an integer between 1 and 90"
                  ) from exc

              if lookback_days < 1 or lookback_days > 90:
                  raise ValueError(
                      "SAVINGS_PLAN_COVERAGE_LOOKBACK_DAYS must be between 1 and 90"
                  )

              end_date = datetime.date.today()
              start_date = end_date - datetime.timedelta(days=lookback_days)

              total_savings = 0.0
              total_cost = 0.0
              next_token = None

              while True:
                  kwargs = {
                      "TimePeriod": {
                          "Start": start_date.isoformat(),
                          "End": end_date.isoformat(),
                      },
                      "Granularity": "DAILY",
                  }
                  if next_token:
                      kwargs["NextToken"] = next_token

                  response = cost_explorer.get_savings_plans_coverage(**kwargs)
                  for coverage in response.get("SavingsPlansCoverages", []):
                      data = coverage.get("Coverage", {})
                      total_savings += parse_float(data.get("SavingsPlansSavings"))
                      total_cost += parse_float(data.get("TotalCost"))

                  next_token = response.get("NextToken")
                  if not next_token:
                      break

              denominator = total_cost + total_savings
              if denominator <= 0:
                  raise ValueError("Savings Plan coverage data is empty for the selected window")

              discount_percent = max(min(total_savings / denominator, 0.9999), 0.0) * 100
              print(
                  "Derived Savings Plan discount percent from Cost Explorer coverage: "
                  f"{discount_percent:.4f}% over the last {lookback_days} days"
              )
              return 1 - (discount_percent / 100.0)

          def get_savings_plan_factor():
              global SAVINGS_PLAN_FACTOR
              global SAVINGS_PLAN_FACTOR_SOURCE
              with SAVINGS_PLAN_LOCK:
                  if SAVINGS_PLAN_FACTOR is not None:
                      return SAVINGS_PLAN_FACTOR

                  mode = os.environ.get("SAVINGS_PLAN_DISCOUNT_MODE", "Manual").strip().lower()

                  if mode == "coverage":
                      try:
                          SAVINGS_PLAN_FACTOR = get_coverage_discount_factor()
                          SAVINGS_PLAN_FACTOR_SOURCE = "coverage"
                          return SAVINGS_PLAN_FACTOR
                      except Exception as coverage_error:
                          print(
                              "Falling back to manual Savings Plan discount due to coverage error: "
                              f"{coverage_error}"
                          )

                  SAVINGS_PLAN_FACTOR = get_manual_discount_factor()
                  SAVINGS_PLAN_FACTOR_SOURCE = "manual"
                  return SAVINGS_PLAN_FACTOR

          def get_metric_namespace():
              raw_value = os.environ.get("SAVINGS_METRIC_NAMESPACE")
              if raw_value is None:
                  return DEFAULT_METRIC_NAMESPACE
              namespace = raw_value.strip()
              return namespace

          def get_downsize_type():
              raw_value = os.environ.get("OFF_HOURS_INSTANCE_TYPE")
              if raw_value:
                  downsized = raw_value.strip()
                  if downsized:
                      return downsized
              return DEFAULT_DOWNSIZE_TYPE

          def publish_savings_metrics(report, timestamp):
              namespace = get_metric_namespace()
              if not namespace:
                  print("Savings metric namespace not configured. Skipping CloudWatch metrics.")
                  return

              metric_data = [
                  {
                      "MetricName": "TotalHourlySavings",
                      "Dimensions": [
                          {"Name": "Region", "Value": report["region"]},
                      ],
                      "Timestamp": timestamp,
                      "Value": report["total_hourly_savings"],
                      "Unit": "None",
                  }
              ]

              projected_total = report.get("projected_total_savings")
              if projected_total is not None:
                  metric_data.append(
                      {
                          "MetricName": "TotalProjectedOffHoursSavings",
                          "Dimensions": [
                              {"Name": "Region", "Value": report["region"]},
                          ],
                          "Timestamp": timestamp,
                          "Value": projected_total,
                          "Unit": "None",
                      }
                  )

              projected_duration = report.get("projected_off_hours_duration_hours")
              if projected_duration is not None:
                  metric_data.append(
                      {
                          "MetricName": "ProjectedOffHoursDurationHours",
                          "Dimensions": [
                              {"Name": "Region", "Value": report["region"]},
                          ],
                          "Timestamp": timestamp,
                          "Value": projected_duration,
                          "Unit": "None",
                      }
                  )

              for instance in report["instances"]:
                  metric_data.append(
                      {
                          "MetricName": "InstanceHourlySavings",
                          "Dimensions": [
                              {"Name": "Region", "Value": report["region"]},
                              {"Name": "InstanceId", "Value": instance["instance_id"]},
                          ],
                          "Timestamp": timestamp,
                          "Value": instance["hourly_savings"],
                          "Unit": "None",
                      }
                  )

              for i in range(0, len(metric_data), 20):
                  batch = metric_data[i : i + 20]
                  cloudwatch.put_metric_data(Namespace=namespace, MetricData=batch)

          def build_actual_savings_snapshot(instance_id, tags, desired_type, current_type):
              last_down_value = tags.get(LAST_SCALE_DOWN_TIMESTAMP_TAG)
              hourly_value = tags.get(LAST_SCALE_DOWN_HOURLY_TAG)

              if not last_down_value or not hourly_value:
                  print(
                      "Missing scale-down metadata for "
                      f"{instance_id}. Skipping actual savings calculation."
                  )
                  return None

              scale_down_time = parse_timestamp(last_down_value)
              if not scale_down_time:
                  print(
                      "Unable to parse scale-down timestamp for "
                      f"{instance_id}: {last_down_value}"
                  )
                  return None

              last_scale_up_value = tags.get(LAST_SCALE_UP_TIMESTAMP_TAG)
              last_scale_up_time = parse_timestamp(last_scale_up_value)
              if last_scale_up_time and last_scale_up_time >= scale_down_time:
                  print(
                      "Actual savings already recorded after the last scale-down "
                      f"window for {instance_id}. Skipping."
                  )
                  return None

              try:
                  hourly_savings = float(hourly_value)
              except ValueError:
                  print(
                      "Invalid hourly savings tag for "
                      f"{instance_id}: {hourly_value}"
                  )
                  return None

              scale_up_time = datetime.datetime.utcnow().replace(microsecond=0)
              downtime_hours = max(
                  (scale_up_time - scale_down_time).total_seconds() / 3600.0,
                  0.0,
              )
              actual_savings_value = round(hourly_savings * downtime_hours, 4)

              print(
                  f"Measured downtime for {instance_id}: {downtime_hours:.4f} hours, "
                  f"actual savings ${actual_savings_value:.4f}"
              )

              record = {
                  "instance_id": instance_id,
                  "off_hours_type": current_type,
                  "restored_type": desired_type,
                  "scale_down_timestamp": format_utc(scale_down_time),
                  "scale_up_timestamp": format_utc(scale_up_time),
                  "downtime_hours": round(downtime_hours, 4),
                  "hourly_savings": round(hourly_savings, 4),
                  "actual_savings": actual_savings_value,
                  "actual_savings_source": f"tag:{LAST_SCALE_DOWN_HOURLY_TAG}",
              }

              return record, scale_up_time

          def publish_actual_savings_metrics(report, timestamp):
              namespace = get_metric_namespace()
              if not namespace:
                  print("Savings metric namespace not configured. Skipping CloudWatch metrics.")
                  return

              metric_data = [
                  {
                      "MetricName": "TotalActualSavings",
                      "Dimensions": [
                          {"Name": "Region", "Value": report["region"]},
                      ],
                      "Timestamp": timestamp,
                      "Value": report["total_actual_savings"],
                      "Unit": "None",
                  },
                  {
                      "MetricName": "TotalActualDowntimeHours",
                      "Dimensions": [
                          {"Name": "Region", "Value": report["region"]},
                      ],
                      "Timestamp": timestamp,
                      "Value": report["total_actual_downtime_hours"],
                      "Unit": "None",
                  },
              ]

              hourly_basis = report.get("total_hourly_savings_basis")
              if hourly_basis is not None:
                  metric_data.append(
                      {
                          "MetricName": "TotalActualHourlySavingsBasis",
                          "Dimensions": [
                              {"Name": "Region", "Value": report["region"]},
                          ],
                          "Timestamp": timestamp,
                          "Value": hourly_basis,
                          "Unit": "None",
                      }
                  )

              for instance in report.get("instances", []):
                  instance_id = instance.get("instance_id")
                  if not instance_id:
                      continue
                  actual_savings = instance.get("actual_savings")
                  if actual_savings is not None:
                      metric_data.append(
                          {
                              "MetricName": "InstanceActualSavings",
                              "Dimensions": [
                                  {"Name": "Region", "Value": report["region"]},
                                  {"Name": "InstanceId", "Value": instance_id},
                              ],
                              "Timestamp": timestamp,
                              "Value": actual_savings,
                              "Unit": "None",
                          }
                      )
                  downtime_value = instance.get("downtime_hours")
                  if downtime_value is not None:
                      metric_data.append(
                          {
                              "MetricName": "InstanceDowntimeHours",
                              "Dimensions": [
                                  {"Name": "Region", "Value": report["region"]},
                                  {"Name": "InstanceId", "Value": instance_id},
                              ],
                              "Timestamp": timestamp,
                              "Value": downtime_value,
                              "Unit": "None",
                          }
                      )

              for i in range(0, len(metric_data), 20):
                  batch = metric_data[i : i + 20]
                  cloudwatch.put_metric_data(Namespace=namespace, MetricData=batch)

          def record_savings(savings_records):
              run_time = get_run_start_time() or datetime.datetime.utcnow()
              run_time = run_time.replace(microsecond=0)
              timestamp = format_utc(run_time)
              region = get_region()
              savings_plan_factor = get_savings_plan_factor()
              total_hourly_savings = round(
                  sum(r["hourly_savings"] for r in savings_records), 4
              ) if savings_records else 0.0

              summary = {
                  "timestamp": timestamp,
                  "region": region,
                  "savings_plan_discount_percent": round(
                      (1 - savings_plan_factor) * 100, 4
                  ),
                  "savings_plan_discount_source": SAVINGS_PLAN_FACTOR_SOURCE,
                  "total_hourly_savings": total_hourly_savings,
                  "instances": savings_records,
              }

              try:
                  projection_details = compute_projected_savings(
                      total_hourly_savings,
                      run_time,
                  )
              except Exception as projection_error:
                  print(
                      "Unable to derive projected savings window: "
                      f"{projection_error}"
                  )
                  summary["projection_error"] = str(projection_error)
              else:
                  summary.update(projection_details)

              if not savings_records:
                  summary["zero_savings"] = True
                  summary["zero_savings_reason"] = "no_savings_records_generated"
                  summary["zero_savings_flags"] = ["no_savings_records"]

              log_event = {
                  "level": "INFO",
                  "event": "SavingsRunSummary",
                  "summary": summary,
              }
              print(json.dumps(log_event))

              key = f"savings/{run_time.date()}/{timestamp}.json"
              bucket = os.environ.get("SAVINGS_BUCKET")
              if not bucket:
                  raise ValueError("SAVINGS_BUCKET environment variable is not set")
              print(f"Writing savings report to s3://{bucket}/{key}")
              s3.put_object(
                  Bucket=bucket,
                  Key=key,
                  Body=json.dumps(summary, indent=2).encode("utf-8"),
              )

              if savings_records:
                  publish_savings_metrics(summary, run_time)

          def record_actual_savings(actual_records):
              run_time = get_run_start_time() or datetime.datetime.utcnow()
              run_time = run_time.replace(microsecond=0)
              timestamp = format_utc(run_time)
              region = get_region()

              total_actual_savings = round(
                  sum(record.get("actual_savings", 0.0) for record in actual_records),
                  4,
              ) if actual_records else 0.0
              total_downtime_hours = round(
                  sum(record.get("downtime_hours", 0.0) for record in actual_records),
                  4,
              ) if actual_records else 0.0
              total_hourly_basis = round(
                  sum(record.get("hourly_savings", 0.0) for record in actual_records),
                  4,
              ) if actual_records else 0.0

              summary = {
                  "timestamp": timestamp,
                  "region": region,
                  "calculation_source": "instance_tag_metadata",
                  "total_actual_savings": total_actual_savings,
                  "total_actual_downtime_hours": total_downtime_hours,
                  "total_hourly_savings_basis": total_hourly_basis,
                  "instances": actual_records,
              }

              if not actual_records:
                  summary["zero_savings"] = True
                  summary["zero_savings_reason"] = "no_actual_savings_records"
                  summary["zero_savings_flags"] = ["no_actual_records"]

              log_event = {
                  "level": "INFO",
                  "event": "ActualSavingsSummary",
                  "summary": summary,
              }
              print(json.dumps(log_event))

              key = f"actual-savings/{run_time.date()}/{timestamp}.json"
              bucket = os.environ.get("SAVINGS_BUCKET")
              if not bucket:
                  raise ValueError("SAVINGS_BUCKET environment variable is not set")
              print(f"Writing actual savings report to s3://{bucket}/{key}")
              s3.put_object(
                  Bucket=bucket,
                  Key=key,
                  Body=json.dumps(summary, indent=2).encode("utf-8"),
              )

              if actual_records:
                  publish_actual_savings_metrics(summary, run_time)

          def process_instance(instance_data, action, run_start_timestamp, downsize_type=None):
              instance = instance_data["instance"]
              instance_id = instance_data["instance_id"]
              current_type = instance_data["current_type"]
              state = instance_data["state"]
              tags = dict(instance_data["tags"])
              ec2_client = get_ec2_client()

              savings_record = None
              actual_record = None
              actual_snapshot_params = None

              print(f"\nProcessing {instance_id} ({state}, {current_type})")

              if action == "scaledown":
                  desired_type = downsize_type or get_downsize_type()
                  if current_type == desired_type:
                      print("Already at downsized type. Skipping.")
                      return False, None, None

                  preferred_type = (tags.get("PreferredInstanceType") or "").strip()
                  if preferred_type:
                      print(
                          "PreferredInstanceType tag already set to "
                          f"{preferred_type}. Preserving existing value and continuing with downsizing."
                      )
                  else:
                      print(f"Tagging {instance_id} with PreferredInstanceType = {current_type}")
                      ec2_client.create_tags(
                          Resources=[instance_id],
                          Tags=[{"Key": "PreferredInstanceType", "Value": current_type}]
                      )
                      tags["PreferredInstanceType"] = current_type

                  try:
                      pricing_filters, pricing_source = get_instance_pricing_profile(instance)
                      print(
                          "Using pricing profile for "
                          f"{instance_id}: {pricing_filters} (source={pricing_source})"
                      )
                      original_rate = get_hourly_rate(current_type, pricing_filters)
                      downsized_rate = get_hourly_rate(desired_type, pricing_filters)
                      hourly_savings = max(original_rate - downsized_rate, 0)
                      hourly_savings *= get_savings_plan_factor()
                      record = {
                          "instance_id": instance_id,
                          "previous_type": current_type,
                          "downsized_type": desired_type,
                          "pricing_operating_system": pricing_filters["operatingSystem"],
                          "pricing_preinstalled_software": pricing_filters["preInstalledSw"],
                          "pricing_license_model": pricing_filters["licenseModel"],
                          "pricing_profile_source": pricing_source,
                          "hourly_savings": round(hourly_savings, 4),
                          "scale_down_timestamp": run_start_timestamp,
                      }
                      savings_record = record
                      ec2_client.create_tags(
                          Resources=[instance_id],
                          Tags=[
                              {
                                  "Key": LAST_SCALE_DOWN_TIMESTAMP_TAG,
                                  "Value": run_start_timestamp,
                              },
                              {
                                  "Key": LAST_SCALE_DOWN_HOURLY_TAG,
                                  "Value": f"{hourly_savings:.4f}",
                              },
                          ],
                      )
                      print(
                          "Recorded scale-down metadata for "
                          f"{instance_id}: timestamp={run_start_timestamp}, "
                          f"hourly_savings=${hourly_savings:.4f}"
                      )
                      print(
                          f"Estimated hourly savings for {instance_id}: ${hourly_savings:.4f}"
                      )
                  except Exception as pricing_error:
                      print(
                          "Unable to calculate savings for "
                          f"{instance_id}: {pricing_error}"
                      )

                  desired_type_value = desired_type

              else:
                  desired_type_value = (tags.get("PreferredInstanceType") or "").strip()
                  if not desired_type_value:
                      print("No PreferredInstanceType tag found. Skipping.")
                      return False, None, None
                  if current_type == desired_type_value:
                      print("Already at desired type. Skipping.")
                      return False, None, None

                  actual_snapshot_params = {
                      "instance_id": instance_id,
                      "tags": tags,
                      "desired_type": desired_type_value,
                      "current_type": current_type,
                  }

              if state != "stopped":
                  print(f"Stopping {instance_id}...")
                  ec2_client.stop_instances(InstanceIds=[instance_id])
                  ec2_client.get_waiter("instance_stopped").wait(InstanceIds=[instance_id])
                  print(f"{instance_id} stopped.")
              else:
                  print(f"{instance_id} already stopped.")

              print(f"Modifying {instance_id} to {desired_type_value}...")
              ec2_client.modify_instance_attribute(
                  InstanceId=instance_id,
                  InstanceType={"Value": desired_type_value}
              )
              wait_for_instance_type(ec2_client, instance_id, desired_type_value)

              print(f"Starting {instance_id}...")
              ec2_client.start_instances(InstanceIds=[instance_id])
              ec2_client.get_waiter("instance_running").wait(InstanceIds=[instance_id])
              print(f"{instance_id} is running.")

              if action == "scaleup" and actual_snapshot_params is not None:
                  actual_result = build_actual_savings_snapshot(
                      **actual_snapshot_params
                  )
                  if actual_result:
                      actual_record, actual_snapshot_time = actual_result
                      scale_up_timestamp_value = format_utc(actual_snapshot_time)
                      ec2_client.create_tags(
                          Resources=[instance_id],
                          Tags=[
                              {
                                  "Key": LAST_SCALE_UP_TIMESTAMP_TAG,
                                  "Value": scale_up_timestamp_value,
                              }
                          ],
                      )
                      print(
                          f"Recorded scale-up metadata for {instance_id}: "
                          f"timestamp={scale_up_timestamp_value}"
                      )

              return True, savings_record, actual_record

          def lambda_handler(event, context):
              action = event.get("action")
              source = event.get("source", "manual")
              schedule_name = normalize_schedule_name(event.get("schedule"))

              if action not in VALID_ACTIONS:
                  raise ValueError(f"Invalid or missing 'action'. Must be one of: {VALID_ACTIONS}")

              if source == "manual":
                  raise Exception("Manual execution is blocked. Use EventBridge scheduled rules.")

              run_start_time = datetime.datetime.utcnow().replace(microsecond=0)
              set_run_start_time(run_start_time)
              run_start_timestamp = format_utc(run_start_time)

              print(
                  f"Starting EC2 {action} process for schedule '{schedule_name}'..."
              )

              ec2_client = get_ec2_client()
              filters = [
                  {"Name": "tag:DynamicInstanceScaling", "Values": ["true"]},
                  {"Name": "instance-state-name", "Values": ["running", "stopped"]},
              ]

              reservations = []
              paginator = ec2_client.get_paginator("describe_instances")
              for page in paginator.paginate(Filters=filters):
                  reservations.extend(page.get("Reservations", []))

              if not reservations:
                  print(
                      "No EC2 instances matched DynamicInstanceScaling=true in running/stopped states. "
                      "Skipping stop/modify/start workflow and savings recording."
                  )
                  return {
                      "processed_instances": 0,
                      "action": action,
                      "schedule": schedule_name,
                      "skipped_instances": 0,
                  }

              instances_to_process = []
              skipped_due_to_schedule = 0

              for reservation in reservations:
                  for instance in reservation.get("Instances", []):
                      instance_id = instance.get("InstanceId")
                      current_type = instance.get("InstanceType")
                      state = (instance.get("State") or {}).get("Name", "unknown")
                      tags = {t["Key"]: t["Value"] for t in instance.get("Tags", [])}

                      if not instance_matches_schedule(tags, schedule_name):
                          skipped_due_to_schedule += 1
                          print(
                              f"Skipping {instance_id} because schedule tag did not match '{schedule_name}'."
                          )
                          continue

                      instances_to_process.append(
                          {
                              "instance": instance,
                              "instance_id": instance_id,
                              "current_type": current_type,
                              "state": state,
                              "tags": tags,
                          }
                      )

              if not instances_to_process:
                  print(
                      "No EC2 instances matched the requested schedule. Skipping workflow."
                  )
                  return {
                      "processed_instances": 0,
                      "action": action,
                      "schedule": schedule_name,
                      "skipped_instances": skipped_due_to_schedule,
                  }

              concurrency = get_concurrency_limit()
              print(
                  f"Processing {len(instances_to_process)} instances with concurrency={concurrency} "
                  f"for schedule '{schedule_name}'."
              )

              downsize_type = get_downsize_type() if action == "scaledown" else None
              savings_records = []
              actual_savings_records = []
              processed_count = 0

              with ThreadPoolExecutor(max_workers=concurrency) as executor:
                  future_to_instance = {
                      executor.submit(
                          process_instance,
                          instance_data,
                          action,
                          run_start_timestamp,
                          downsize_type,
                      ): instance_data["instance_id"]
                      for instance_data in instances_to_process
                  }

                  for future in as_completed(future_to_instance):
                      instance_id = future_to_instance[future]
                      try:
                          processed, savings_record, actual_record = future.result()
                      except Exception as exc:
                          print(f"Error processing {instance_id}: {exc}")
                          raise

                      if processed:
                          processed_count += 1
                      if savings_record:
                          savings_records.append(savings_record)
                      if actual_record:
                          actual_savings_records.append(actual_record)

              if skipped_due_to_schedule:
                  print(
                      f"Skipped {skipped_due_to_schedule} instance(s) due to schedule mismatch."
                  )

              print("Lambda execution completed.")

              if action == "scaledown":
                  record_savings(savings_records)
              else:
                  record_actual_savings(actual_savings_records)

              return {
                  "processed_instances": processed_count,
                  "action": action,
                  "schedule": schedule_name,
                  "skipped_instances": skipped_due_to_schedule,
              }

  EC2ScalerScheduleDown:
    Type: AWS::Events::Rule
    DependsOn: EC2ScalerFunction
    Properties:
      Name: EC2ScalerScheduleDown
      Description: Triggers Lambda to scale down EC2 instances at 7 PM Pacific, Monday through Friday
      ScheduleExpression: !Ref LambdaScheduleDownTime
      State: ENABLED
      Targets:
        - Arn: !GetAtt EC2ScalerFunction.Arn
          Id: DownTarget
          Input: '{"action": "scaledown", "source": "eventbridge", "schedule": "default"}'

  EC2ScalerScheduleUp:
    Type: AWS::Events::Rule
    DependsOn: EC2ScalerFunction
    Properties:
      Name: EC2ScalerScheduleUp
      Description: Triggers Lambda to scale up EC2 instances at 4 AM Pacific, Monday through Friday
      ScheduleExpression: !Ref LambdaScheduleUpTime
      State: ENABLED
      Targets:
        - Arn: !GetAtt EC2ScalerFunction.Arn
          Id: UpTarget
          Input: '{"action": "scaleup", "source": "eventbridge", "schedule": "default"}'

  EC2ScalerBusinessHoursScheduleDown:
    Type: AWS::Events::Rule
    DependsOn: EC2ScalerFunction
    Properties:
      Name: EC2ScalerBusinessHoursScheduleDown
      Description: Triggers Lambda to scale down EC2 instances at 6 PM Pacific, Monday through Friday
      ScheduleExpression: !Ref BusinessHoursScheduleDownTime
      State: ENABLED
      Targets:
        - Arn: !GetAtt EC2ScalerFunction.Arn
          Id: BusinessHoursDownTarget
          Input: '{"action": "scaledown", "source": "eventbridge", "schedule": "business-hours"}'

  EC2ScalerBusinessHoursScheduleUp:
    Type: AWS::Events::Rule
    DependsOn: EC2ScalerFunction
    Properties:
      Name: EC2ScalerBusinessHoursScheduleUp
      Description: Triggers Lambda to scale up EC2 instances at 9 AM Pacific, Monday through Friday
      ScheduleExpression: !Ref BusinessHoursScheduleUpTime
      State: ENABLED
      Targets:
        - Arn: !GetAtt EC2ScalerFunction.Arn
          Id: BusinessHoursUpTarget
          Input: '{"action": "scaleup", "source": "eventbridge", "schedule": "business-hours"}'

  SavingsLogBucket:
    Type: AWS::S3::Bucket

  LambdaInvokePermissionDown:
    Type: AWS::Lambda::Permission
    DependsOn: EC2ScalerScheduleDown
    Properties:
      FunctionName: !Ref EC2ScalerFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EC2ScalerScheduleDown.Arn

  LambdaInvokePermissionUp:
    Type: AWS::Lambda::Permission
    DependsOn: EC2ScalerScheduleUp
    Properties:
      FunctionName: !Ref EC2ScalerFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EC2ScalerScheduleUp.Arn

  LambdaInvokePermissionBusinessHoursDown:
    Type: AWS::Lambda::Permission
    DependsOn: EC2ScalerBusinessHoursScheduleDown
    Properties:
      FunctionName: !Ref EC2ScalerFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EC2ScalerBusinessHoursScheduleDown.Arn

  LambdaInvokePermissionBusinessHoursUp:
    Type: AWS::Lambda::Permission
    DependsOn: EC2ScalerBusinessHoursScheduleUp
    Properties:
      FunctionName: !Ref EC2ScalerFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt EC2ScalerBusinessHoursScheduleUp.Arn
